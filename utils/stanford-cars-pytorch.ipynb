{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms, models\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.nn import Parameter\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, ExponentialLR\n","\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","from scipy.special import binom\n","\n","import cv2\n","import os\n","import math\n","import time\n","import datetime\n","import warnings\n","\n","# from efficientnet_pytorch import EfficientNet"],"execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Configuration\n","\n","pd.set_option('display.max_colwidth', 200)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# files path\n","train_path = \"../input/stanford-cars-dataset/cars_train/cars_train\"\n","test_path = \"../input/stanford-cars-dataset/cars_test/cars_test\"\n","\n","csv_path = '../input/stanford-cars-csv-annotation'\n","\n","csv_train = 'train.csv'\n","csv_test = 'test.csv'"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train = pd.read_csv(os.path.join(csv_path, csv_train))\n","test = pd.read_csv(os.path.join(csv_path, csv_test))\n","\n","train.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   bbox_x1  bbox_y1  bbox_x2  bbox_y2  Category   filename  \\\n0       39      116      569      375        14  00001.jpg   \n1       36      116      868      587         3  00002.jpg   \n2       85      109      601      381        91  00003.jpg   \n3      621      393     1484     1096       134  00004.jpg   \n4       14       36      133       99       106  00005.jpg   \n\n                            class_name  Labels  \n0                  Audi TTS Coupe 2012      13  \n1                  Acura TL Sedan 2012       2  \n2           Dodge Dakota Club Cab 2007      90  \n3     Hyundai Sonata Hybrid Sedan 2012     133  \n4  Ford F-450 Super Duty Crew Cab 2012     105  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bbox_x1</th>\n      <th>bbox_y1</th>\n      <th>bbox_x2</th>\n      <th>bbox_y2</th>\n      <th>Category</th>\n      <th>filename</th>\n      <th>class_name</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>116</td>\n      <td>569</td>\n      <td>375</td>\n      <td>14</td>\n      <td>00001.jpg</td>\n      <td>Audi TTS Coupe 2012</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>36</td>\n      <td>116</td>\n      <td>868</td>\n      <td>587</td>\n      <td>3</td>\n      <td>00002.jpg</td>\n      <td>Acura TL Sedan 2012</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>85</td>\n      <td>109</td>\n      <td>601</td>\n      <td>381</td>\n      <td>91</td>\n      <td>00003.jpg</td>\n      <td>Dodge Dakota Club Cab 2007</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>621</td>\n      <td>393</td>\n      <td>1484</td>\n      <td>1096</td>\n      <td>134</td>\n      <td>00004.jpg</td>\n      <td>Hyundai Sonata Hybrid Sedan 2012</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14</td>\n      <td>36</td>\n      <td>133</td>\n      <td>99</td>\n      <td>106</td>\n      <td>00005.jpg</td>\n      <td>Ford F-450 Super Duty Crew Cab 2012</td>\n      <td>105</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["class CARS(Dataset):\n","    \n","    def __init__(self, dataframe, train=True, transform=None):\n","        self.df = dataframe\n","        self.train = train\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        \n","        label = self.df.Labels.values[idx]\n","        filename = self.df.filename.values[idx]\n","        \n","        if self.train:\n","            image_path = train_path\n","        else:\n","            image_path = test_path\n","        \n","        p_path = os.path.join(image_path, filename)\n","        \n","        image = cv2.imread(p_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = transforms.ToPILImage()(image)\n","        \n","        if self.transform is not None:\n","            image = self.transform(image)\n","        \n","        return image, label"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["mean = (0.4708, 0.4602, 0.4550)\n","sd = (0.2626, 0.2618, 0.2667)\n","train_transform = transforms.Compose([transforms.Resize((448, 448)),\n","                                      transforms.RandomHorizontalFlip(),\n","                                      transforms.ToTensor()                                    \n","])\n","\n","test_transform = transforms.Compose([transforms.Resize((448, 448)),\n","                                     transforms.ToTensor()\n","])\n","\n","\n","trainset = CARS(train, train=True, transform=train_transform)\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch, shuffle=True, num_workers=4)\n","\n","testset = CARS(test, train=False, transform=test_transform)\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch, shuffle=False, num_workers=4)"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class ArcMarginProduct(nn.Module):\n","    r\"\"\"Implement of large margin arc distance: :\n","        Args:\n","            in_features: size of each input sample\n","            out_features: size of each output sample\n","            s: norm of input feature\n","            m: margin\n","\n","            cos(theta + m)\n","        \"\"\"\n","    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n","        super(ArcMarginProduct, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.s = s\n","        self.m = m\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","        self.easy_margin = easy_margin\n","        self.cos_m = math.cos(m)\n","        self.sin_m = math.sin(m)\n","        self.th = math.cos(math.pi - m)\n","        self.mm = math.sin(math.pi - m) * m\n","\n","    def forward(self, input, label):\n","        # --------------------------- cos(theta) & phi(theta) ---------------------------\n","        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n","        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n","        phi = cosine * self.cos_m - sine * self.sin_m\n","        if self.easy_margin:\n","            phi = torch.where(cosine > 0, phi, cosine)\n","        else:\n","            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n","        # --------------------------- convert label to one-hot ---------------------------\n","        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n","        one_hot = torch.zeros(cosine.size(), device='cuda')\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n","        output *= self.s\n","        # print(output)\n","\n","        return cosine, output\n","    \n","class CfeatureExtraction(nn.Module):\n","\n","    def __init__(self):\n","        super(CfeatureExtraction, self).__init__()\n","        xfeatures = models.densenet161(pretrained=True)\n","        self.xfeatures = nn.Sequential(*list(xfeatures.features)[ : ])\n","        \n","        # if self.classifier.bias is not None:\n","        #     nn.init.constant_(self.classifier.bias.data, val=0)\n","        \n","    \"\"\" Training CNN with Arcface loss \"\"\"\n","    def forward(self, inputs: torch.Tensor):\n","        \n","        features = self.xfeatures(inputs)               # extract features from pretrained base\n","        features = F.avg_pool2d(features, kernel_size=features.size()[2: ])\n","        features = features.view(features.size()[0], -1)\n","        return features\n","\n","class fconnected(nn.Module):\n","\n","    def __init__(self):\n","        super(fconnected, self).__init__()\n","        self.fc = nn.Linear(2208, 588)\n","        self.bnorm = nn.BatchNorm1d(588)\n","\n","        nn.init.kaiming_normal_(self.fc.weight.data)\n","\n","    def forward(self, inputs: torch.Tensor):\n","\n","        logit = self.fc(inputs)\n","        logit = self.bnorm(logit)\n","        return logit\n","\n","# models = Net(num_classes = num_classes).to(device)\n","# models(torch.randn(5, 3, 448, 448).to(device)).shape"],"execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["tune_lr = 0.0001\n","base_lr = 0.001\n","weight_decay = 5e-4\n","num_classes = 196\n","\n","F_Extraction = CfeatureExtraction().to(device)\n","# numel_list = [p.numel() for p in F_Extraction.parameters()]\n","# print(sum(numel_list))\n","F_Connected = fconnected().to(device)\n","Arcface = ArcMarginProduct(588, num_classes, s=64, m=0.5).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","Extraction_opt = torch.optim.SGD(F_Extraction.parameters(), lr=tune_lr, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n","Extraction_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(Extraction_opt, mode='max', factor=0.1, patience=4, min_lr=1e-7, verbose=True)\n","\n","fullycon_opt = torch.optim.SGD(F_Connected.parameters(), lr=base_lr, momentum=0.9, nesterov=True)\n","fullycon_scheduler = torch.optim.lr_scheduler.MultiStepLR(fullycon_opt, milestones=[30,50], gamma=0.1)\n","\n","Arcface_opt = torch.optim.SGD(Arcface.parameters(), lr=base_lr, momentum=0.9, nesterov=True)\n","Arcface_scheduler = torch.optim.lr_scheduler.MultiStepLR(Arcface_opt, milestones=[30,50], gamma=0.1)"],"execution_count":8,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/checkpoints/densenet161-8d451a50.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=115730790.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d2134e9c1446b28ec040a9f0f17e6b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["epochs = 60\n","save_model_path = 'Checkpoint'\n","steps = 0\n","running_loss = 0\n","\n","print('Start fine-tuning...')\n","best_acc = 0.\n","best_epoch = None\n","end_patient = 0\n","\n","for epoch in range(epochs):\n","    \n","    start_time = time.time()\n","    for idx, data in enumerate(train_loader):\n","        steps += 1\n","        \n","        # Move input and label tensors to the default device\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        Extraction_opt.zero_grad()\n","        fullycon_opt.zero_grad()\n","        Arcface_opt.zero_grad()\n","        \n","        feature = F_Extraction(inputs)\n","        feature = F_Connected(feature)\n","        _, output = Arcface(feature, labels)\n","        loss = criterion(output, labels)\n","\n","        loss.backward()\n","        Extraction_opt.step()\n","        fullycon_opt.step()\n","        Arcface_opt.step()\n","\n","        running_loss += loss.item()\n","    stop_time = time.time()\n","    print('Epoch {}/{} and used time: {:.4f} sec.'.format(epoch+1, epochs, stop_time - start_time))\n","    \n","    F_Extraction.eval(), F_Connected.eval(), Arcface.eval()\n","    for name, loader in [(\"train\", train_loader), (\"test\", test_loader)]:\n","        _acc = 0\n","        _loss = 0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for data in loader:\n","                \n","                imgs, labels = data\n","                imgs, labels = imgs.to(device), labels.to(device)\n","                \n","                feature = F_Extraction(imgs)\n","                feature = F_Connected(feature)\n","                logit, output = Arcface(feature, labels)\n","                loss = criterion(output, labels)\n","                _loss += loss.item()\n","                \n","                result = F.softmax(logit, dim=1)\n","                _, predicted = torch.max(result, dim=1)\n","                \n","                total += labels.shape[0]\n","                correct += int((predicted == labels).sum())\n","            _acc = 100 * correct  / total\n","            _loss = _loss / len(loader)\n","            \n","        print('{} loss: {:.6f}    {} accuracy: {:.4f}'.format(name, _loss, name, _acc))\n","    print()\n","    \n","\n","    running_loss = 0\n","    F_Extraction.train(), F_Connected.train(), Arcface.train()\n","    Extraction_scheduler.step(_acc)\n","    fullycon_scheduler.step(_acc)\n","    Arcface_scheduler.step(_acc)\n","    \n","    if _acc > best_acc:\n","#         model_file = os.path.join(save_model_path, 'resnet34_CUB_200_fine_tuning_epoch_{}_acc_{}.pth'.format(best_epoch, best_acc))\n","        \n","#         if os.path.isfile(model_file):\n","#             os.remove(os.path.join(save_model_path, 'resnet34_CUB_200_fine_tuning_epoch_{}_acc_{}.pth'.format(best_epoch, best_acc)))\n","        \n","        end_patient = 0\n","        best_acc = _acc\n","        best_epoch = epoch + 1\n","#         print('The accuracy is improved, save model')\n","#         torch.save(model.state_dict(), os.path.join(save_model_path,'resnet34_CUB_200_fine_tuning_epoch_{}_acc_{}.pth'.format(best_epoch, best_acc)))\n","        \n","    else:\n","        end_patient += 1\n","\n","    if end_patient >= 10 and epoch > 50:\n","        break\n","print('After the training, the end of the epoch {}, the highest accuracy is {:.2f}'.format(best_epoch, best_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":"Start fine-tuning...\nEpoch 1/60 and used time: 434.0897 sec.\ntrain loss: 22.368288    train accuracy: 83.8040\ntest loss: 25.304214    test accuracy: 74.2072\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"Epoch 2/60 and used time: 432.6561 sec.\ntrain loss: 14.360408    train accuracy: 95.5550\ntest loss: 19.047729    test accuracy: 86.0092\n\nEpoch 3/60 and used time: 433.2686 sec.\ntrain loss: 9.606579    train accuracy: 97.7775\ntest loss: 15.426206    test accuracy: 89.2426\n\nEpoch 4/60 and used time: 434.0155 sec.\ntrain loss: 6.672475    train accuracy: 98.2932\ntest loss: 13.157254    test accuracy: 90.8718\n\nEpoch 5/60 and used time: 435.9317 sec.\ntrain loss: 4.555559    train accuracy: 98.8581\ntest loss: 11.665681    test accuracy: 91.8667\n\nEpoch 6/60 and used time: 433.6046 sec.\ntrain loss: 2.950754    train accuracy: 99.2878\ntest loss: 10.422315    test accuracy: 92.5009\n\nEpoch 7/60 and used time: 434.5586 sec.\ntrain loss: 1.873774    train accuracy: 99.3861\ntest loss: 9.656757    test accuracy: 92.9362\n\nEpoch 8/60 and used time: 435.6669 sec.\ntrain loss: 1.146389    train accuracy: 99.5948\ntest loss: 9.107306    test accuracy: 93.5829\n\nEpoch 9/60 and used time: 436.2263 sec.\ntrain loss: 0.643371    train accuracy: 99.7299\ntest loss: 8.614977    test accuracy: 93.7321\n\nEpoch 10/60 and used time: 436.5739 sec.\ntrain loss: 0.402769    train accuracy: 99.7544\ntest loss: 8.597052    test accuracy: 93.8316\n\nEpoch 11/60 and used time: 436.0836 sec.\ntrain loss: 0.217042    train accuracy: 99.8527\ntest loss: 8.119816    test accuracy: 94.2793\n\nEpoch 12/60 and used time: 437.1188 sec.\ntrain loss: 0.151916    train accuracy: 99.8772\ntest loss: 7.996401    test accuracy: 94.2420\n\nEpoch 13/60 and used time: 437.8446 sec.\ntrain loss: 0.120868    train accuracy: 99.8404\ntest loss: 7.738962    test accuracy: 94.2296\n\nEpoch 14/60 and used time: 436.8679 sec.\ntrain loss: 0.121513    train accuracy: 99.8281\ntest loss: 7.701246    test accuracy: 94.1923\n\nEpoch 15/60 and used time: 436.1430 sec.\ntrain loss: 0.098748    train accuracy: 99.8527\ntest loss: 7.599167    test accuracy: 94.3042\n\nEpoch 16/60 and used time: 433.8387 sec.\ntrain loss: 0.100234    train accuracy: 99.8649\ntest loss: 7.426224    test accuracy: 94.3539\n\nEpoch 17/60 and used time: 435.7284 sec.\ntrain loss: 0.101433    train accuracy: 99.8404\ntest loss: 7.283058    test accuracy: 94.1923\n\nEpoch 18/60 and used time: 436.7256 sec.\ntrain loss: 0.093665    train accuracy: 99.8649\ntest loss: 7.249137    test accuracy: 94.2171\n\nEpoch 19/60 and used time: 437.1775 sec.\ntrain loss: 0.087412    train accuracy: 99.8772\ntest loss: 7.363577    test accuracy: 94.4410\n\nEpoch 20/60 and used time: 437.2412 sec.\ntrain loss: 0.096960    train accuracy: 99.8527\ntest loss: 7.173876    test accuracy: 94.3166\n\nEpoch 21/60 and used time: 437.8398 sec.\ntrain loss: 0.083473    train accuracy: 99.8772\ntest loss: 7.229288    test accuracy: 94.2793\n\nEpoch 22/60 and used time: 439.9522 sec.\ntrain loss: 0.094026    train accuracy: 99.8527\ntest loss: 7.112148    test accuracy: 94.1674\n\nEpoch 23/60 and used time: 436.5866 sec.\ntrain loss: 0.098267    train accuracy: 99.8404\ntest loss: 7.165935    test accuracy: 94.4161\n\nEpoch 24/60 and used time: 463.7533 sec.\ntrain loss: 0.087209    train accuracy: 99.8649\ntest loss: 7.045269    test accuracy: 94.4410\n\nEpoch    24: reducing learning rate of group 0 to 1.0000e-05.\nEpoch 25/60 and used time: 451.2415 sec.\ntrain loss: 0.087531    train accuracy: 99.8527\ntest loss: 6.913214    test accuracy: 94.5529\n\nEpoch 26/60 and used time: 433.8127 sec.\ntrain loss: 0.095981    train accuracy: 99.8404\ntest loss: 6.848275    test accuracy: 94.4907\n\nEpoch 27/60 and used time: 432.8548 sec.\ntrain loss: 0.087425    train accuracy: 99.8527\ntest loss: 6.828999    test accuracy: 94.5902\n\nEpoch 28/60 and used time: 434.2846 sec.\ntrain loss: 0.110273    train accuracy: 99.8158\ntest loss: 6.814586    test accuracy: 94.5280\n\nEpoch 29/60 and used time: 433.5748 sec.\ntrain loss: 0.095513    train accuracy: 99.8527\ntest loss: 6.779241    test accuracy: 94.5280\n\nEpoch 30/60 and used time: 435.1243 sec.\ntrain loss: 0.084006    train accuracy: 99.8649\ntest loss: 6.793894    test accuracy: 94.4907\n\nEpoch 31/60 and used time: 432.7755 sec.\ntrain loss: 0.100657    train accuracy: 99.8404\ntest loss: 6.735151    test accuracy: 94.6524\n\nEpoch 32/60 and used time: 433.4013 sec.\ntrain loss: 0.112526    train accuracy: 99.8158\ntest loss: 6.721488    test accuracy: 94.5654\n\nEpoch 33/60 and used time: 433.1896 sec.\ntrain loss: 0.095654    train accuracy: 99.8527\ntest loss: 6.740341    test accuracy: 94.6151\n\nEpoch 34/60 and used time: 433.0741 sec.\ntrain loss: 0.070202    train accuracy: 99.8895\ntest loss: 6.693084    test accuracy: 94.6648\n\nEpoch 35/60 and used time: 432.7052 sec.\ntrain loss: 0.107696    train accuracy: 99.8281\ntest loss: 6.706619    test accuracy: 94.5032\n\nEpoch 36/60 and used time: 437.1129 sec.\ntrain loss: 0.098827    train accuracy: 99.8281\ntest loss: 6.699803    test accuracy: 94.3788\n\nEpoch 37/60 and used time: 435.6706 sec.\ntrain loss: 0.069669    train accuracy: 99.8895\ntest loss: 6.689968    test accuracy: 94.5529\n\nEpoch 38/60 and used time: 434.8208 sec.\ntrain loss: 0.074188    train accuracy: 99.8895\ntest loss: 6.670471    test accuracy: 94.5529\n\nEpoch 39/60 and used time: 435.0409 sec.\ntrain loss: 0.081172    train accuracy: 99.8649\ntest loss: 6.679207    test accuracy: 94.6027\n\nEpoch    39: reducing learning rate of group 0 to 1.0000e-06.\nEpoch 40/60 and used time: 435.6145 sec.\ntrain loss: 0.073806    train accuracy: 99.8772\ntest loss: 6.712952    test accuracy: 94.5529\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}